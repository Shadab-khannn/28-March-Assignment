{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87546e5-8129-4f66-a848-e486dea63f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85fe09d-37b2-41ae-977d-fb551cc2b177",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9a2b4-4132-4f4f-a397-a88f0d423c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regression is a type of linear regression that uses L2 regularization to prevent overfitting. It differs from ordinary least squares\n",
    "regression in the following ways:\n",
    "\n",
    "1.Ridge regression produces a lower test mean squared error compared to least squares regression when multicollinearity is present.\n",
    "2.Ridge regression uses a ridge estimator to estimate the coefficients, which is biased but has lower variance than the OLS estimator.\n",
    "3.Ridge regression tries to find the coefficients that minimize the mean squared error and wants the magnitude of coefficients to be as small\n",
    "  as possible.\n",
    "4.Ridge regression adds just enough bias to make the estimates reasonably reliable approximations to true population values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ee1ef-1be4-4855-97b1-0d41390fc4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801b7cf-30c1-4672-963a-28801477a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39d2fb-bd99-412b-97ef-ab0ba97e4994",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab4cc5-8e28-40a3-837d-389795067d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The assumptions of Ridge Regression are the same as that of linear regression: linearity, constant variance, and independence. \n",
    "However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumeD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f076fef-727f-4753-81b9-429deb28fd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d7905-cb31-4d46-880d-946cb9d64749",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364dfa9-613a-4289-9eaa-97e8f4412631",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d19e92-1952-40ec-bc17-cead5e846fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "The best value for lambda in ridge regression can be chosen by cross-validation. One can check a bunch of different values on training data \n",
    "and see which yields the best R2 on test data. Information criteria such as AIC and BIC can also be used to select lambda.\n",
    "\n",
    "In ridge regression, we add a penalty by way of a tuning parameter called lambda which is chosen using cross-validation. The idea is to\n",
    "make the fit small by making the residual sum or squares small plus adding a shrinkage penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04245afc-5dd4-4cf6-9d5f-a44065be3500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9981b8-fd07-4480-b4fc-c7aa70d3b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583664d-0069-4b64-a93b-5da2bf901162",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b70e0d-448d-47c2-92cf-31c42fe0755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge Regression can be used for feature selection. Ridge regression is a regularization technique that shrinks the regression coefficients\n",
    "towards zero. It can be used to reduce the impact of irrelevant features in the model. The L2 penalty term in Ridge Regression helps in\n",
    "reducing the magnitude of the coefficients of the features that are not important for the model. This way, Ridge Regression can be used for\n",
    "feature selection.\n",
    "\n",
    "In addition to Ridge Regression, there are other regularization techniques like Lasso Regression and Elastic Net that can also be used for \n",
    "feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3f2dc-a62d-4480-a8f6-de8d11fb270f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdda9e0-aa3e-486e-9b34-34f40dd752ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ef610-f955-4829-9226-be6ab94b4363",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0ea0d-db72-48b8-9f09-d022c5e222c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regression is a model tuning method that is used to analyze any data that suffers from multicollinearity. When the issue of\n",
    "multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the \n",
    "actual values. Ridge regression performs L2 regularization which adds a penalty equal to the square of the magnitude of coefficients. \n",
    "This penalty term helps to reduce the coefficients of variables that are highly correlated with other variables. This helps to reduce the\n",
    "variance of the estimates and thus improve the prediction accuracy of the model.\n",
    "\n",
    "In summary, Ridge regression is a useful technique for analyzing models containing multicollinearity. It helps to reduce the variance of \n",
    "estimates and improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e5db7-110b-4032-8896-4b73c4a9db35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028aaf88-e00b-4d45-8766-88400a10e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867caf8-7855-459a-8ecf-4fec238f4905",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dda705-361c-4887-93de-6699518191b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regression is used for regression purpose only as it needs the dependent variable to be continuous. So, Ridge regression canâ€™t be \n",
    "used with categorical variables. However, when the dependent variable is categorical, you can state the problem as classification and use\n",
    "logistic regression instead.\n",
    "\n",
    "If you have more than one category, you can make them all binary. For example, if you have apple, orange, pear, instead of saying 1,2,3 \n",
    "say is_apple = [0,1] is_orange= [0,1] is_pear= [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b061c-f5fb-40db-8b7e-0f070b7f324e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b23492-9199-4003-95e6-4f7d1e2bf7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e47b4-56de-40de-b214-ae124e210ed2",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9928c34-11f9-49e2-9773-836553b57067",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Ridge Regression, the coefficients are shrunk towards zero by adding a penalty term to the loss function. The penalty term is\n",
    "proportional to the square of the magnitude of the coefficients. The larger the value of the penalty parameter, the greater the amount \n",
    "of shrinkage. The coefficients are shrunk towards zero but not exactly zero. This means that Ridge Regression does not perform variable\n",
    "selection but rather it reduces the impact of less important variables on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702dda7-6aba-41c6-8fcf-2cce548fceef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f0ada-279b-4fc9-b455-8bbf729185e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51a4fc-ebb8-4808-8cbb-37e6941c84cf",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3eb75-eeb9-49b4-901d-5a70d7976669",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Ridge Regression can be used for time-series data analysis. In time series regression, the dependent variable is a time series, \n",
    "and the independent variables can be other time series or non-time series variables. Time series regression helps you understand the \n",
    "relationship between variables over time and forecast future values of the dependent variable.\n",
    "\n",
    "Ridge regression is a regularized linear regression method that can be used to analyze time-series data. It is used to prevent overfitting \n",
    "in linear regression models by adding a penalty term to the loss function. The penalty term is proportional to the square of the magnitude \n",
    "of the coefficients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
